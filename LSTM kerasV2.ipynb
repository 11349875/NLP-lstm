{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651eb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from timeit import default_timer as timer\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from keras.models import model_from_json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6c817",
   "metadata": {},
   "source": [
    "Only run this code if you have a GPU. This part of the code makes the code run on your GPU, I used the tensorflow-gpu version 1.15 for this to work. It's considerably faster using my GPU (NVIDIA GTX 1060 6GB), than my CPU. About ~5 times faster, depending on the dataset I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8de405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:27:00.0, compute capability: 6.1\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11139154771864956303\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5083824128\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18103136064573924285\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:27:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d86a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0aed777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"brown.csv\")\n",
    "data = ' '.join(df['tokenized_text'].tolist()).replace(\"\\'\", \"\")\n",
    "dataval = data[1518586:1700000]\n",
    "data = data[0:1518586]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8623aa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Furthermore , as an encouragement to revisionist thinking , it manifestly is fair to admit that any fraternity has a constitutional right to refuse to accept persons it dislikes . The Unitarian clergy were an exclusive club of cultivated gentlemen -- as the term was then understood in the Back Bay -- and Parker was definitely not a gentleman , either in theology or in manners . Ezra Stiles Gannett , an honorable representative of the sanhedrin , addressed himself frankly to the issue in 1845 , insisting that Parker should not be persecuted or calumniated and that in this republic no power to restrain him by force could exist . Even so , Gannett judiciously argued , the Association could legitimately decide that Parker `` should not be encouraged nor assisted in diffusing his opinions by those who differ from him in regard to their correctness  . We today are not entitled to excoriate honest men who believed Parker to be downright pernicious and who barred their pulpits against his dema'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff03120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128865\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"jokes.csv\")\n",
    "data = ' '.join(df['Joke'].tolist()).replace(\"\\'\", \"\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6105f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180713\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/46/46-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text\n",
    "data = data[1353:]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "616a6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797205\n"
     ]
    }
   ],
   "source": [
    "url = \"http://gutenberg.org/files/1342/1342-0.txt\"\n",
    "book = requests.get(url)\n",
    "data1 = book.text\n",
    "data1 = data1[2440:]\n",
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be15dfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202023"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://gutenberg.org/files/65084/65084-0.txt\"\n",
    "book = requests.get(url)\n",
    "data2 = book.text\n",
    "data2 = data2[2974:]\n",
    "data2 = data2[:-34443]\n",
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e588eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129658\n"
     ]
    }
   ],
   "source": [
    "url = \"http://gutenberg.org/files/1250/1250-0.txt\"\n",
    "book = requests.get(url)\n",
    "data3 = book.text\n",
    "data3 = data3[1197:]\n",
    "print(len(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0216d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652062\n"
     ]
    }
   ],
   "source": [
    "url = \"http://gutenberg.org/files/521/521-0.txt\"\n",
    "book = requests.get(url)\n",
    "data4 = book.text\n",
    "data4 = data4[1883:]\n",
    "print(len(data4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2618acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data + data1 + data2 + data3 + data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feb04136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1518586"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f7f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c1147",
   "metadata": {},
   "source": [
    "Cleaning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5627c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "\n",
    "    dataset = ''.join(i for i in dataset if not i.isdigit())\n",
    "    dataset = dataset.lower()\n",
    "    dataset = dataset.replace(\"Ã¢\", \"\")\n",
    "    dataset = dataset.replace(\"_\", \"\")\n",
    "    dataset = dataset.replace(\"--\", \" \")\n",
    "    dataset = dataset.replace(\"*\", \"\")\n",
    "    \n",
    "    # remove whitespace\n",
    "    tokenized = word_tokenize(dataset)\n",
    "    dataset = \" \".join([token.strip() for token in tokenized])\n",
    "\n",
    "    # remove characters\n",
    "    dataset = re.sub(r\"[^\\w\\n]\", \" \", dataset)\n",
    "\n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    dataset = re.sub(r\"\\s+\", \" \", dataset)\n",
    "    dataset = dataset.strip()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data = clean_dataset(data)\n",
    "dataval = clean_dataset(dataval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59fe0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'furthermore as an encouragement to revisionist thinking it manifestly is fair to admit that any fraternity has a constitutional right to refuse to accept persons it dislikes the unitarian clergy were an exclusive club of cultivated gentlemen as the term was then understood in the back bay and parker was definitely not a gentleman either in theology or in manners ezra stiles gannett an honorable representative of the sanhedrin addressed himself frankly to the issue in insisting that parker should not be persecuted or calumniated and that in this republic no power to restrain him by force could exist even so gannett judiciously argued the association could legitimately decide that parker should not be encouraged nor assisted in diffusing his opinions by those who differ from him in regard to their correctness we today are not entitled to excoriate honest men who believed parker to be downright pernicious and who barred their pulpits against his demand to poison the minds of their congreg'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da69efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing and he thought\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "# test set for measuring perplexity\n",
    "test_num = int(0.9 * len(data))\n",
    "testdata = data[test_num:len(data)]\n",
    "data = data[0:test_num]\n",
    "\n",
    "testdata = testdata.split()\n",
    "testseqs = []\n",
    "\n",
    "sequence_length = 4\n",
    "seq_length = sequence_length\n",
    "\n",
    "for i in range(sequence_length, len(testdata)):\n",
    "    sequence = testdata[i-sequence_length:i+1]\n",
    "    testseqs.append(sequence)\n",
    "\n",
    "testseqs = np.array(testseqs)\n",
    "\n",
    "XD = []\n",
    "for x in testseqs:\n",
    "    XD.append(' '.join(x))\n",
    "    \n",
    "testseqs = XD\n",
    "\n",
    "# divide the sequence into data and target\n",
    "testseqs = np.array(testseqs)\n",
    "\n",
    "Xtest = []\n",
    "Ytest = []\n",
    "for sequence in testseqs:\n",
    "    Xtest.append(' '.join(sequence.split()[0:-1]))\n",
    "    Ytest.append(sequence.split()[-1])\n",
    "    \n",
    "print(Xtest[0])\n",
    "print(Ytest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb15830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292564"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4013c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furthermore as an encouragement\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "# measure perplexity on train data\n",
    "testdata = data[0:int((0.1*len(data)))]\n",
    "testdata = testdata.split()\n",
    "testseqs = []\n",
    "\n",
    "for i in range(sequence_length, len(testdata)):\n",
    "    sequence = testdata[i-sequence_length:i+1]\n",
    "    testseqs.append(sequence)\n",
    "\n",
    "testseqs = np.array(testseqs)\n",
    "\n",
    "XD = []\n",
    "for x in testseqs:\n",
    "    XD.append(' '.join(x))\n",
    "    \n",
    "testseqs = XD\n",
    "\n",
    "# divide the sequence into data and target\n",
    "testseqs = np.array(testseqs)\n",
    "\n",
    "Xtrainperplex = []\n",
    "Ytrainperplex = []\n",
    "for sequence in testseqs:\n",
    "    Xtrainperplex.append(' '.join(sequence.split()[0:-1]))\n",
    "    Ytrainperplex.append(sequence.split()[-1])\n",
    "    \n",
    "print(Xtrainperplex[0])\n",
    "print(Ytrainperplex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f5bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  20023\n"
     ]
    }
   ],
   "source": [
    "#Tokenize train dataset\n",
    "word_tokeniser = Tokenizer()\n",
    "word_tokeniser.fit_on_texts([data])\n",
    "words_embedding = word_tokeniser.texts_to_sequences([data])[0]\n",
    "vocab_size = len(word_tokeniser.word_index) + 1\n",
    "print(\"Vocabulary Size: \", vocab_size)\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for i in range(sequence_length, len(words_embedding)):\n",
    "    sequence = words_embedding[i-sequence_length:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# divide the sequence into data and target\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:427736,:-1]  # assign all but last words of a sequence to X\n",
    "y = sequences[:427736,-1]   # assign last word of each sequence to y\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "X = pad_sequences(X, maxlen=seq_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0959fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation data\n",
    "# Couldn't there be words in the validation data that hasn't been seen yet? What will happen?\n",
    "# Also using vocab_size of train set, for correct input size\n",
    "testdata = dataval\n",
    "\n",
    "words_embedding1 = word_tokeniser.texts_to_sequences([testdata])[0]\n",
    "\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for i in range(sequence_length, len(words_embedding1)):\n",
    "    sequence = words_embedding1[i-sequence_length:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "\n",
    "# divide the sequence into data and target\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "Xvalid = sequences[:227736,:-1]  # assign all but last words of a sequence to X\n",
    "yvalid = sequences[:227736,-1]   # assign last word of each sequence to y\n",
    "yvalid = to_categorical(yvalid, num_classes=vocab_size)\n",
    "Xvalid = pad_sequences(Xvalid, maxlen=seq_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f93df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 200, input_length = seq_length))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24884138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "226927/226927 [==============================] - 68s 299us/step - loss: 7.3339 - categorical_crossentropy: 7.3339\n",
      "Loss and accuracy on valid\n",
      "[6.760691001057835, 6.760693550109863]\n",
      "Saved model to disk\n",
      "\n",
      "Epoch 1/1\n",
      "226927/226927 [==============================] - 76s 336us/step - loss: 6.7533 - categorical_crossentropy: 6.7533\n",
      "Loss and accuracy on valid\n",
      "[6.526186622404337, 6.526185512542725]\n",
      "Saved model to disk\n",
      "\n",
      "Epoch 1/1\n",
      "226927/226927 [==============================] - 82s 360us/step - loss: 6.3997 - categorical_crossentropy: 6.3997\n",
      "Loss and accuracy on valid\n",
      "[6.44957383288698, 6.4495744705200195]\n",
      "Saved model to disk\n",
      "\n",
      "Epoch 1/1\n",
      "  7424/226927 [..............................] - ETA: 59s - loss: 6.0524 - categorical_crossentropy: 6.0524 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7d490e365c84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-7d490e365c84>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochsPer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochsPer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochsPer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss and accuracy on valid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\a\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\a\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0;32m    185\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[1;32m~\\anaconda3\\envs\\a\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\a\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, epochs, batch_size):\n",
    "    \n",
    "    epochsPer = int(epochs/10)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    json_file.close()\n",
    "\n",
    "    for i in range(epochsPer, epochs+1, epochsPer):\n",
    "        model.fit(X, y, epochs=epochsPer, verbose=1, batch_size=256)\n",
    "        print(\"Loss and accuracy on valid\")\n",
    "        print(model.evaluate(x=Xvalid, y=yvalid, batch_size=128, verbose=0))\n",
    "        \n",
    "\n",
    "        model.save_weights(str(i) + \"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        print(\"\")\n",
    "    return model\n",
    "\n",
    "\n",
    "        \n",
    "model = train_model(model, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e066b4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.39051994438104, 0.09809523820877075]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=Xvalid, y=yvalid, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dac1921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"decentmodel.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0d5ea",
   "metadata": {},
   "source": [
    "Make predictions using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e730a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(model, word_tokeniser, seq_length, text, n_words):\n",
    "    for x in range(n_words):\n",
    "        \n",
    "        # create word embeddings\n",
    "        words_embedding = word_tokeniser.texts_to_sequences([text])[0]\n",
    "\n",
    "        padded_words = pad_sequences([words_embedding], maxlen=seq_length, padding='pre')\n",
    "        # predict next word\n",
    "        print(padded_words)\n",
    "        prediction = model.predict_classes(padded_words, verbose=0)\n",
    "\n",
    "        print(sorted(model.predict(padded_words)[0], reverse=True)[0:5])\n",
    "\n",
    "        next_word = \"\"\n",
    "        for word, i in word_tokeniser.word_index.items():\n",
    "            if [i] == prediction:\n",
    "                next_word = word\n",
    "                break\n",
    "        text += \" \" + next_word\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae905114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of their claim to\n",
      "the\n",
      "[[   2   40 1874    4]]\n",
      "[0.2308572, 0.073507994, 0.069345936, 0.06533595, 0.05622109]\n",
      "of their claim to come\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_words = 1\n",
    "i = random.randint(0,1000)\n",
    "sentence = Xtrainperplex[i]\n",
    "print(sentence)\n",
    "print(Ytrainperplex[i])\n",
    "print(generate_words(model, word_tokeniser, seq_length, sentence, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6958d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.406625802556724e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1567.0408897674083"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline_perplex(data):\n",
    "    distribution = defaultdict(int)\n",
    "    for word in data.split():\n",
    "        distribution[word] += 1\n",
    "        \n",
    "    probdistribution = []   \n",
    "    sumD = sum(distribution.values())\n",
    "    for num in list(distribution.values()):\n",
    "        probdistribution.append(num/sumD)\n",
    "        \n",
    "    count = 0\n",
    "    for key in distribution.keys():\n",
    "        distribution[key] = probdistribution[count]\n",
    "        count += 1\n",
    "\n",
    "    chance = 0\n",
    "    print((1/sumD))\n",
    "    for i in range(len(Ytest)):\n",
    "        chance += math.log2(distribution.get(Ytest[i], (1/sumD)))\n",
    "    return 2**((-1/len(Xtest))* chance)\n",
    "\n",
    "\n",
    "baseline_perplex(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0979cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplex_help(model, word_tokeniser, seq_length, text, real_word):\n",
    "        # create word embeddings\n",
    "        words_embedding = word_tokeniser.texts_to_sequences([text])[0]\n",
    "        # gets ID of the real word, and checks the probility assigned to it.\n",
    "        # If the word hasn't been seen by the model before, I just make it 'the'\n",
    "        padded_words = pad_sequences([words_embedding], maxlen=seq_length, padding='pre')\n",
    "        idx = word_tokeniser.word_index.get(real_word, word_tokeniser.word_index['the'])\n",
    "        prediction = model.predict_classes(padded_words, verbose=0)\n",
    "\n",
    "        for word, i in word_tokeniser.word_index.items():\n",
    "            if [i] == prediction:\n",
    "                print(word)\n",
    "        return model.predict(padded_words)[0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3784235c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ing and he thought\n",
      "it\n",
      "entirely\n",
      "5.1999343e-08\n",
      "\n",
      "and he thought it\n",
      "lovely\n",
      "would\n",
      "1.1070911e-20\n",
      "\n",
      "he thought it lovely\n",
      "he\n",
      "knowledge\n",
      "3.1282432e-06\n",
      "\n",
      "thought it lovely he\n",
      "didnt\n",
      "guess\n",
      "9.513626e-05\n",
      "\n",
      "it lovely he didnt\n",
      "doubt\n",
      "feel\n",
      "1.1404793e-17\n",
      "\n",
      "lovely he didnt doubt\n",
      "her\n",
      "he\n",
      "0.003692627\n",
      "\n",
      "he didnt doubt her\n",
      "truthfulness\n",
      "he\n",
      "1.36601575e-05\n",
      "\n",
      "didnt doubt her truthfulness\n",
      "although\n",
      "she\n",
      "2.1627971e-05\n",
      "\n",
      "doubt her truthfulness although\n",
      "he\n",
      "they\n",
      "0.046781205\n",
      "\n",
      "her truthfulness although he\n",
      "had\n",
      "himself\n",
      "0.034977753\n",
      "\n",
      "truthfulness although he had\n",
      "heard\n",
      "become\n",
      "0.001339142\n",
      "\n",
      "although he had heard\n",
      "the\n",
      "so\n",
      "0.009500773\n",
      "\n",
      "he had heard the\n",
      "words\n",
      "elevator\n",
      "0.00084658794\n",
      "\n",
      "had heard the words\n",
      "a\n",
      "up\n",
      "0.02494165\n",
      "\n",
      "heard the words a\n",
      "hundred\n",
      "cent\n",
      "2.090845e-05\n",
      "\n",
      "the words a hundred\n",
      "times\n",
      "year\n",
      "0.00034990546\n",
      "\n",
      "words a hundred times\n",
      "ive\n",
      "district\n",
      "5.7246884e-11\n",
      "\n",
      "a hundred times ive\n",
      "never\n",
      "gone\n",
      "0.0012558465\n",
      "\n",
      "hundred times ive never\n",
      "done\n",
      "married\n",
      "0.00021779229\n",
      "\n",
      "times ive never done\n",
      "this\n",
      "she\n",
      "0.008964911\n",
      "\n",
      "ive never done this\n",
      "before\n",
      "would\n",
      "1.5144275e-05\n",
      "\n",
      "never done this before\n",
      "they\n",
      "me\n",
      "0.017746808\n",
      "\n",
      "done this before they\n",
      "always\n",
      "found\n",
      "0.0001410839\n",
      "\n",
      "this before they always\n",
      "said\n",
      "handled\n",
      "0.0004205833\n",
      "\n",
      "before they always said\n",
      "shaking\n",
      "had\n",
      "5.331252e-13\n",
      "\n",
      "they always said shaking\n",
      "their\n",
      "and\n",
      "0.00027322254\n",
      "\n",
      "always said shaking their\n",
      "dresses\n",
      "head\n",
      "3.8105768e-13\n",
      "\n",
      "said shaking their dresses\n",
      "down\n",
      "second\n",
      "1.5858175e-06\n",
      "\n",
      "shaking their dresses down\n",
      "over\n",
      "carrier\n",
      "6.0131706e-06\n",
      "\n",
      "their dresses down over\n",
      "their\n",
      "a\n",
      "0.28988913\n",
      "\n",
      "dresses down over their\n",
      "white\n",
      "foil\n",
      "1.1791542e-05\n",
      "\n",
      "down over their white\n",
      "shoulders\n",
      "prokofieff\n",
      "5.3687972e-06\n",
      "\n",
      "over their white shoulders\n",
      "ive\n",
      "who\n",
      "1.0810051e-08\n",
      "\n",
      "their white shoulders ive\n",
      "never\n",
      "seen\n",
      "0.005991744\n",
      "\n",
      "white shoulders ive never\n",
      "done\n",
      "got\n",
      "0.00018034928\n",
      "\n",
      "shoulders ive never done\n",
      "this\n",
      "but\n",
      "0.0001511486\n",
      "\n",
      "ive never done this\n",
      "before\n",
      "would\n",
      "1.5144275e-05\n",
      "\n",
      "never done this before\n",
      "they\n",
      "me\n",
      "0.017746808\n",
      "\n",
      "done this before they\n",
      "always\n",
      "found\n",
      "0.0001410839\n",
      "\n",
      "this before they always\n",
      "said\n",
      "handled\n",
      "0.0004205833\n",
      "\n",
      "before they always said\n",
      "waiting\n",
      "had\n",
      "1.5635846e-05\n",
      "\n",
      "they always said waiting\n",
      "for\n",
      "for\n",
      "0.38779247\n",
      "\n",
      "always said waiting for\n",
      "the\n",
      "it\n",
      "0.009003546\n",
      "\n",
      "said waiting for the\n",
      "elevator\n",
      "old\n",
      "1.6411689e-06\n",
      "\n",
      "waiting for the elevator\n",
      "in\n",
      "in\n",
      "0.7115975\n",
      "\n",
      "for the elevator in\n",
      "the\n",
      "his\n",
      "0.03055585\n",
      "\n",
      "the elevator in the\n",
      "hotel\n",
      "flannagans\n",
      "3.488619e-05\n",
      "\n",
      "elevator in the hotel\n",
      "corridor\n",
      "year\n",
      "2.1891724e-16\n",
      "\n",
      "in the hotel corridor\n",
      "ive\n",
      "areas\n",
      "6.033379e-14\n",
      "\n",
      "the hotel corridor ive\n",
      "never\n",
      "written\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-319558803db6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mchance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mperplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-319558803db6>\u001b[0m in \u001b[0;36mperplexity\u001b[1;34m(Xtest, Ytest)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mwordprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperplex_help\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_tokeniser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwordprob\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mchance\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-93b45063cad3>\u001b[0m in \u001b[0;36mperplex_help\u001b[1;34m(model, word_tokeniser, seq_length, text, real_word)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokeniser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def perplexity(Xtest, Ytest):\n",
    "    chance = 0\n",
    "    for i in range(len(Xtest)):\n",
    "        print(\"\")\n",
    "        print(Xtest[i])\n",
    "        print(Ytest[i])\n",
    "        wordprob = perplex_help(model, word_tokeniser, seq_length, Xtest[i], Ytest[i])\n",
    "        if wordprob != 0:\n",
    "            chance += math.log2(wordprob)\n",
    "        \n",
    "        print(wordprob)\n",
    "    return 2**((-1/len(Xtest))* chance)\n",
    "\n",
    "perplexity(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c174f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

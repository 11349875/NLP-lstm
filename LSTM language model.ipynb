{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c1af30",
   "metadata": {
    "id": "651eb072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU, LSTM\n",
    "from keras.layers import Embedding\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from keras.models import model_from_json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import collections\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation is at the bottom of the notebook\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39d49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text and do some final cleaning\n",
    "\n",
    "# Change text to desired text\n",
    "text = '1000000_0_0'\n",
    "\n",
    "file = 'CLEAN_' + text + '.txt'\n",
    "path = 'data/' + text + '/'\n",
    "\n",
    "with open(path + file, encoding=\"utf8\") as handle:\n",
    "    sentences =  [l.strip()[0:-1] for l in handle.readlines()]\n",
    "    \n",
    "sentences1 = []\n",
    "for sent in sentences:\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    sentences1.append(sent.strip())\n",
    "\n",
    "x = []\n",
    "\n",
    "for i in sentences1:\n",
    "    if len(i.split()) > 2 and len(i.split()) < 50:\n",
    "        x.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1979ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentencepiece model\n",
    "sp = spm.SentencePieceProcessor(model_file=path + 'encoding.model')\n",
    "sp.SetEncodeExtraOptions('bos:eos')\n",
    "\n",
    "# vocab size\n",
    "v = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072bd310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557644\n"
     ]
    }
   ],
   "source": [
    "# Encode sentences, and split them the way I describe in my thesis\n",
    "\n",
    "X = []\n",
    "X1 = []\n",
    "y = []\n",
    "for sentence in x:\n",
    "    count = 1\n",
    "    temp = sp.encode(sentence)\n",
    "    for subword in temp:\n",
    "        X.append(temp[0:count])\n",
    "        if count < len(temp):\n",
    "            y.append(temp[count])\n",
    "        count += 1\n",
    "        \n",
    "for A in X:\n",
    "    if 2 not in A:\n",
    "        X1.append(A)\n",
    "\n",
    "X = X1\n",
    "y = to_categorical(y, num_classes=v)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c7a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21647507\n",
      "1458245\n",
      "1458245\n"
     ]
    }
   ],
   "source": [
    "# Remove datapoints with more than 40 subwords\n",
    "count = 0\n",
    "max1 = 40\n",
    "X1 = []\n",
    "y1 = []\n",
    "\n",
    "for XD in range(len(X)):\n",
    "    if len(X[XD]) < max1:\n",
    "        count += len(X[XD])\n",
    "        X1.append(X[XD])\n",
    "        y1.append(y[XD])\n",
    "        \n",
    "X = X1\n",
    "y = y1\n",
    "print(count)\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d6d833",
   "metadata": {
    "id": "fd1eb0e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458245, 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences\n",
    "\n",
    "X1 = []\n",
    "for sent in X:\n",
    "    X1.append(pad_sequences([sent], maxlen=max1, padding='pre'))\n",
    "    \n",
    "X2 = []\n",
    "for sent in X1:\n",
    "    X2.append(sent[0])\n",
    "\n",
    "X = X2\n",
    "X2 = []\n",
    "X1 = []\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8413456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Split the train and test data with a ratio of 1 to 4\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "X_train = X[:int(len(X)*0.8)]\n",
    "X_test = X[int(len(X)*0.8):]\n",
    "X = []\n",
    "\n",
    "y_train = y[:int(len(y)*0.8)]\n",
    "y_test = y[int(len(y)*0.8):]\n",
    "y = []\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a31618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "np.set_printoptions(precision=None, suppress=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad22cea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# My own grid search implementation\n",
    "print(text)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1, restore_best_weights=True)\n",
    "\n",
    "batches = [256]\n",
    "dropout = [0.1, 0.2, 0.4]\n",
    "lr = [0.001, 0.002]\n",
    "layer_size = [256]\n",
    "\n",
    "# Meshgrid creates arrays of every possible combinations\n",
    "grid = np.array(np.meshgrid(batches,dropout,lr, layer_size)).T.reshape(-1, 4)\n",
    "\n",
    "# for every combination in grid...\n",
    "for element in grid:\n",
    "    batch = int(element[0])\n",
    "    layer = int(element[3])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=element[2], beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(v, 200, input_length = max1))\n",
    "    \n",
    "    # if CuDNN doesnt work, try: model.add(LSTM(layer))\n",
    "    model.add(CuDNNLSTM(layer, return_sequences=True))\n",
    "    model.add(Dropout(element[1]))\n",
    "    model.add(CuDNNLSTM(layer))\n",
    "    model.add(Dense(v, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    print(\"Now fitting parameters: \" + str(element))\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=100, verbose=1, batch_size=batch, callbacks=[early_stopping], validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de3f8116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load a model from the data to generate text\n",
    "# Only have to change text name to desired text\n",
    "text = '1000000_2_5'\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file= 'data/'+ text+ '/encoding.model')\n",
    "sp.SetEncodeExtraOptions('bos:eos')\n",
    "json_file = open('data/' +text + '/model/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('data/' +text + '/model/modelcheckP.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a34b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate words deterministically (no randomness), can often get stuck in a loop, especially with the non-zipfian trained models\n",
    "# Appends a '.' when the model predicts an EOS.\n",
    "\n",
    "max1 = 40\n",
    "\n",
    "def generate_words_deter(model, text, num):\n",
    "    for _ in range(num):\n",
    "        padded_words = pad_sequences([text], maxlen=max1, padding='pre')\n",
    "        a = np.argsort(model.predict_proba(padded_words)[0])[-2:]\n",
    "        if a[0] == text[-1] or a[0] == 0:\n",
    "            b = int(a[1])\n",
    "        else:\n",
    "            b = int(a[0])\n",
    "            \n",
    "            # Keeps the high typicality models from predicting too many EOS\n",
    "            if b == 2 and b not in text[-4:]:\n",
    "                text.append(2)\n",
    "                text.append(3)\n",
    "                text.append(1)\n",
    "                continue\n",
    "        text.append(b)\n",
    "    return sp.decode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d80e03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate words with a random modifier, that probabillistically picks a word from the top X words.\n",
    "# Appends a '.' when the model predicts an EOS.\n",
    "\n",
    "max1 = 40\n",
    "def generate_words_prob(model, text, num):\n",
    "    for _ in range(num):\n",
    "        \n",
    "        padded_words = pad_sequences([text], maxlen=max1, padding='pre')\n",
    "        abc = model.predict_proba(padded_words)[0]\n",
    "        a = list(reversed(np.argsort(abc)[-4:]))\n",
    "        \n",
    "        weights = sorted(abc, reverse=True)[0:4]\n",
    "        \n",
    "        if 0 in a:\n",
    "            XD = a.index(0)\n",
    "            a.pop(XD)\n",
    "            weights.pop(XD)\n",
    "            \n",
    "        b = int(random.choices(a,weights=weights)[0])\n",
    "        \n",
    "        # Keeps the high typicality models from predicting too many EOS\n",
    "        if b == 2 and b not in text[-4:]:\n",
    "            text.append(2)\n",
    "            text.append(3)\n",
    "            text.append(1)\n",
    "            continue\n",
    "            \n",
    "        text.append(b)\n",
    "\n",
    "    return sp.decode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5c2703e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'den norske byens hovedsy'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can give the model some context to generate from, although it'll go off on a tangent of its own quite quickly\n",
    "\n",
    "sentence = \"den norske byen\" # the Norwegian city\n",
    "sentence = sp.encode(sentence)[:-1] \n",
    "\n",
    "generate_words_deter(model, sentence, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dbdcc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'han ble valgt inn på stortinget i 2006 og spilte en ny tittelstyring i den amerikansk prøyssisk og krigen og var med i det tyske landslaget i usa og ble tildelt de olympisk rekord. dermed i norge og var en av norges mest berømte utøverne i den internasjonale finalen i 1999 og ble slått til fange. den tyske sosialdemokratiet i norge og ble valgt inn på stortingets landsstyr. den lokale arkitekt. ernst bowie is the newss the newss the newss som var med i det amerikansk ungarsk. det amerikansk ungarsk. han ble valgt som en av norges mest berømte bidrag. de to første i norge i den andre. det var i tillegg ikke en selvstendig skikkelse for de avdøde og de fleste av de viktigste protestantisk og i norge og de fleste andre. den hellige stolte i tillegg. det var en viktig delstat. et stort publikumssted. et annet slag. et annet slag. et annet slag. et annet marked i esbrungspark. den andre seremonisstyrken. essens stadt og en annen forening som var i stand fra en annen rekke andre. den andre delene i den tyske kirke i delstaten schanwsburg i tyskland og ble utgitt på singlene. irak og ble utgitt på single. den andre delen i den internasjonale klubben i 2001 og har et areal i den tyske kirke og fylkesdepartementet i oslo og ble en viktig rolle. tiltaket. den nye monarksorganisasjonen. en annen departement for den norske okkupasjonen. det nye rike. det ble i tillegg samlet at den var i bruk med å sikre seg. det som ble i tillegg nominert på 34. en delstat. den hellige grav. den andre esseennolog. en av dem. en av dem som ble gitt til fange av hans morsefare i den tyske kirke i den nye verden i den tyske hæren og var i stand på å gi'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or let the model generate from nothing, and just give it a BOS token\n",
    "\n",
    "generate_words_deter(model, [1], 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b919135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'den norske byen er en av flere'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same thing for probablistic methods\n",
    "\n",
    "sentence = \"den norske byen\" # the Norwegian city\n",
    "sentence = sp.encode(sentence)[:-1] \n",
    "\n",
    "generate_words_prob(model, sentence, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b267357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'han ble kreert til kardinal i 1857 og i 1674. hans hustru ingebrigssy og hans bror av england. den hellige navn. i den første verdenskrig. den andre verdenskrig brøt han i den tyske hæren. den hellige stol. en av de mest populære faktorene i paris. en del av den tyske kirke. den hellige stol. en del av en annen del av en del av den tyske okkupasjonsmakten i den tyske hæren. det ble bygget i 1730. det som var en del av den tyske hæren. den andre verdenskrigen ble bygget. det var en viktig del av de allierte. dermed ble det også en del av det tyske riket. irak og det ble også i. en rekke andre klubber i norge. en del av en rekke andre klubber. en rekke kjente personer som ble utgitt på single. den sørlige delen av fjorden. stein. en av de største idealene.eppes. irak. irakene. en del av en del av en del av den tyske okkupasjonsmakten. den andre verdenskrig brøt ut i et eget departement for den tyske okkupasjonsmakten. den nye okkupasjonsmakten. i tillegg til å organisere et nytt byggområde. en rekke andre klubber. andre land. andre verdenskrig. stiftelsen av den katolske kirke. i dag. en annen delovitsj departementer. yuan. isabele. et annet slag. en rekke andre skip som ble utgitt på singlen. irak. irak. den andre verdenskrig. den sørlige delen av den sørlige delen av verden og i tillegg til å spille et utvalg. en annen del av den tyske okkupasjonen av de allierte i norge. en del av en del av en del av den tyske okkupasjonen. det ble også gitt inn i en rekke land. den lokale domstolen. en annen del av den politiske situasjonen. isbret. en annen del av den politiske regjeringen. den hellige stol. den hellige stol. den andre verdenskrigen. det var en del av de tyske linjene. den andre verdenskrigen. den andre verdenskrigen. en annen seremoni. en annen del av den tyske hæren og i den første kampen ble det bestemt at det var i bruk. en ny del av de allierte i norge. den hellige stol. en annen styrke. en del av en ny del av den nye kongen. den hellige. den hellige stol. den andre verdenskrig. den'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_words_prob(model, [1], 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f15a9c0",
   "metadata": {
    "id": "6958d4ac",
    "outputId": "aefe423c-981d-480c-8790-1f5eef66700d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198.85458981015"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure perplexity of model, you'll need to load all the correct data for this to work\n",
    "# It looks a little complicated because everything had to be done in batches, or else it would crash\n",
    "\n",
    "def perplexity(Xp, yp, model):\n",
    "    chance = 0\n",
    "    current_index = 0\n",
    "    batch_size = 500\n",
    "    rang = int(len(Xp) / batch_size) - 1\n",
    "    length = batch_size * rang\n",
    "    for _ in range(rang):\n",
    "        distributionList = model.predict(Xp[current_index:current_index + batch_size], batch_size)\n",
    "    \n",
    "        tempcount = current_index\n",
    "        for distribution in distributionList:\n",
    "            chance += math.log2(distribution[np.nonzero(yp[tempcount])[0][0]])\n",
    "            tempcount += 1\n",
    "            \n",
    "        current_index = current_index + batch_size \n",
    "    return 2**((-1/length)* chance)\n",
    "\n",
    "perplexity(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49faaf43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

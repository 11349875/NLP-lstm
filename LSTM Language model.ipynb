{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651eb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6c817",
   "metadata": {},
   "source": [
    "Only run this code if you have a GPU. This part of the code makes the code run on your GPU, I used the tensorflow-gpu version 1.15 for this to work. It's considerably faster using my GPU (NVIDIA GTX 1060 6GB), than my CPU. About ~5 times faster, depending on the dataset I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8de405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:27:00.0, compute capability: 6.1\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12005222237026359134\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5083824128\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8974174928950180734\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:27:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48863882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9037f96e",
   "metadata": {},
   "source": [
    "Several different datasets I experimented on, they are listed from short to long. The shortest one takes a couple of seconds to train, the longest one can, depending on your computer, take hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aed777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29088\n"
     ]
    }
   ],
   "source": [
    "with open('wikiped.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff03120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128866\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"jokes.csv\")\n",
    "data = ' '.join(df['Joke'].tolist()).replace(\"\\'\", \"\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "616a6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797205\n"
     ]
    }
   ],
   "source": [
    "url = \"http://gutenberg.org/files/1342/1342-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text\n",
    "data = data[2440:]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be15dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396753\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/24869/24869-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c1147",
   "metadata": {},
   "source": [
    "Cleaning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5627c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset, char_filter = r\"[^\\w]\"):\n",
    "\n",
    "    # convert words to lower case\n",
    "    dataset = dataset.lower()\n",
    "    dataset = dataset.replace(\"Ã¢\", \"a\")\n",
    "    # tokenise words\n",
    "    words = word_tokenize(dataset)\n",
    "\n",
    "    # strip whitespace from all words\n",
    "    words = [word.strip() for word in words]\n",
    "\n",
    "        \n",
    "    # join back words to get dataset\n",
    "    dataset = \" \".join(words)\n",
    "\n",
    "    # remove unwanted characters\n",
    "    dataset = re.sub(char_filter, \" \", dataset)\n",
    "\n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    dataset = re.sub(r\"\\s+\", \" \", dataset)\n",
    "\n",
    "    # strip whitespace from dataset\n",
    "    dataset = dataset.strip()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data = clean_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2b958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife however little known the feelings or views of such a man may be on his first entering a neighbourhood this truth is so well fixed in the minds of the surrounding families that he is considered the rightful property of some one or other of their daughters a my dear mr bennet a said his lady to him one day a have you heard that netherfield park is let at last a mr bennet replied that he had not a but it is a returned she a for mrs long has just been here and she told me all about it a mr bennet made no answer a do you not want to know who has taken it a cried his wife impatiently a _you_ want to tell me and i have no objection to hearing it a this was invitation enough a why my dear you must know mrs long says that netherfield is taken by a young man of large fortune from the north of england that he came down on monday in a chaise and four to see the place and was so much d\n"
     ]
    }
   ],
   "source": [
    "print(data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ab50d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokeniser = Tokenizer()\n",
    "word_tokeniser.fit_on_texts([data])\n",
    "encoded_words = word_tokeniser.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cca6e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 6875\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f5bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "MAX_SEQ_LENGTH = 10\n",
    "\n",
    "for i in range(MAX_SEQ_LENGTH, len(encoded_words)):\n",
    "    sequence = encoded_words[i-MAX_SEQ_LENGTH:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the sequence into X and y\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:80000,:-1]  # assign all but last words of a sequence to X\n",
    "y = sequences[:80000,-1]   # assign last word of each sequence to y\n",
    "y = to_categorical(y, num_classes=VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f92cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f93df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 100)           687500    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 128)           117248    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6875)              886875    \n",
      "=================================================================\n",
      "Total params: 1,823,207\n",
      "Trainable params: 1,823,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model architecture\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH))\n",
    "\n",
    "# lstm layer 1\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(VOCABULARY_SIZE, activation='softmax'))\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24884138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "80000/80000 [==============================] - 9s 118us/step - loss: 0.3257 - accuracy: 0.9356\n",
      "Epoch 2/6\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2517 - accuracy: 0.9582\n",
      "Epoch 3/6\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.2043 - accuracy: 0.9709\n",
      "Epoch 4/6\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.1766 - accuracy: 0.9778\n",
      "Epoch 5/6\n",
      "80000/80000 [==============================] - 9s 115us/step - loss: 0.1693 - accuracy: 0.9789\n",
      "Epoch 6/6\n",
      "80000/80000 [==============================] - 9s 114us/step - loss: 0.1726 - accuracy: 0.9782\n",
      "55.24112939999986\n"
     ]
    }
   ],
   "source": [
    "# The 'joke' database on GPU 154.4100482 seconds for 100 epochs seq length of 4.\n",
    "# The 'joke' database on CPU 439.003922 seconds for 100 epochs seq length of 4.\n",
    "\n",
    "start = timer()\n",
    "model.fit(X, y, epochs=6, verbose=1, batch_size=256)\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0d5ea",
   "metadata": {},
   "source": [
    "Make predictions using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e730a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed, n_words):\n",
    "    \n",
    "    text = seed\n",
    "    \n",
    "    # generate n_words\n",
    "    for _ in range(n_words):\n",
    "        \n",
    "        # encode text as integers\n",
    "        encoded_words = word_tokeniser.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # pad sequences\n",
    "        padded_words = pad_sequences([encoded_words], maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "        \n",
    "        # predict next word\n",
    "        prediction = model.predict_classes(padded_words, verbose=0)\n",
    "        \n",
    "        print(sorted(model.predict(padded_words)[0], reverse=True)[0:10])\n",
    "        \n",
    "        # convert predicted index to its word\n",
    "        next_word = \"\"\n",
    "        for word, i in word_tokeniser.word_index.items():\n",
    "            if i == prediction:\n",
    "                next_word = word\n",
    "                break\n",
    "        \n",
    "        # append predicted word to text\n",
    "        text += \" \" + next_word\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b83f9",
   "metadata": {},
   "source": [
    "Pride and Prejudice corpus results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae905114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99917847, 0.00068758277, 6.306314e-05, 1.7433202e-05, 1.1614113e-05, 8.018825e-06, 5.4203883e-06, 5.2689443e-06, 3.340532e-06, 2.7873257e-06]\n",
      "[0.18028742, 0.17101498, 0.16787572, 0.09690676, 0.09265507, 0.06591675, 0.062038455, 0.042713653, 0.017685033, 0.011922607]\n",
      "[0.19502637, 0.1772658, 0.1566488, 0.121589184, 0.07760419, 0.07511853, 0.05972236, 0.04104813, 0.021070376, 0.014065979]\n",
      "[0.765685, 0.069594964, 0.040247027, 0.018514656, 0.016270895, 0.012419882, 0.010289915, 0.01003431, 0.0062004873, 0.005689317]\n",
      "I was very much flattered by the regiment since mr\n"
     ]
    }
   ],
   "source": [
    "num_words = 4\n",
    "\n",
    "sentence = \"I was very much flattered by\"\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, sentence, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5608bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28368878, 0.14794074, 0.1075356, 0.09502737, 0.08129186, 0.061073847, 0.043049652, 0.03332956, 0.030446982, 0.012883066]\n",
      "[0.7236329, 0.27459297, 0.0012267408, 0.0002207649, 0.00012167183, 5.8653062e-05, 4.212259e-05, 1.758753e-05, 1.7078182e-05, 1.6908958e-05]\n",
      "[0.6196457, 0.15557612, 0.06316139, 0.049632628, 0.033423506, 0.015990576, 0.012141923, 0.007345006, 0.006750378, 0.005515686]\n",
      "though he was now only established as a full of men\n"
     ]
    }
   ],
   "source": [
    "num_words = 3\n",
    "\n",
    "sentence = \"though he was now only established as a\"\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, sentence, num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f499cd",
   "metadata": {},
   "source": [
    "With the joke dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d468fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6683171, 0.25462034, 0.0651055, 0.00431349, 0.0012285054, 0.00091190205, 0.0007116308, 0.00050085026, 0.00046884821, 0.00038164938]\n",
      "[0.98092365, 0.0048741344, 0.0023324254, 0.0018169106, 0.0009334787, 0.00072973984, 0.0006090417, 0.0005649004, 0.00053405005, 0.00049594225]\n",
      "Knock knock whos there\n"
     ]
    }
   ],
   "source": [
    "num_words = 2\n",
    "\n",
    "sentence = \"Knock knock\"\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, sentence, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7123cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18106021, 0.10527202, 0.087918885, 0.06749019, 0.04438426, 0.027411213, 0.02216347, 0.020651741, 0.020146707, 0.019159257]\n",
      "[0.8620607, 0.061113253, 0.012960028, 0.010936391, 0.010249718, 0.006638202, 0.005750539, 0.005358668, 0.003060069, 0.002434764]\n",
      "[0.752341, 0.04861866, 0.04823112, 0.039533246, 0.015625905, 0.013603913, 0.013017845, 0.0076805474, 0.0075942194, 0.0070863375]\n",
      "[0.7690748, 0.110211566, 0.0147162145, 0.014382733, 0.014136069, 0.009521721, 0.008592104, 0.008185829, 0.007048806, 0.005682896]\n",
      "What did the rubber band factory worker\n"
     ]
    }
   ],
   "source": [
    "num_words = 4\n",
    "\n",
    "sentence = \"What did the\"\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, sentence, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca10cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

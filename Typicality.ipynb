{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import json\n",
    "from nltk import tokenize\n",
    "import collections\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel,\\\n",
    "        GenericLikelihoodModelResults\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "from scipy.special import zeta\n",
    "from scipy.stats import binom\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "from scipy.special import zeta\n",
    "from scipy.misc import derivative\n",
    "\n",
    "import numpy as np\n",
    "lg = np.log10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing without part of speech tags\n",
    "def remove_punctuation(text):\n",
    "    text = text.lower()\n",
    "    chars_to_remove = \"[\\n]!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    tr = str.maketrans(\" \", \" \", chars_to_remove)\n",
    "    return text.translate(tr)\n",
    "\n",
    "\n",
    "def preprocess(corpus, sent = True):\n",
    "    if sent:\n",
    "        corpus = tokenize.sent_tokenize(corpus)\n",
    "        corpus = [remove_punctuation(sent).split() for sent in corpus]\n",
    "    else:\n",
    "        corpus = remove_punctuation(corpus).split()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing with part of speech tags\n",
    "def part_of_speech(corpus):\n",
    "    corpus = tokenize.sent_tokenize(corpus)\n",
    "    chars_to_remove = \"[\\n]\"\n",
    "    tr = str.maketrans(\" \", \" \", chars_to_remove)\n",
    "    chars_to_remove2 = \"[\\n]!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    new_corp = []\n",
    "    test = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        sent = sent.translate(tr)\n",
    "        words_sent = tokenize.word_tokenize(sent)\n",
    "        sent_pos = nltk.pos_tag(words_sent)\n",
    "        new_sent = []\n",
    "        for (word, pos) in sent_pos:\n",
    "            tr2 = str.maketrans(\"\", \"\", chars_to_remove2)\n",
    "            word = word.translate(tr2)\n",
    "            if word:\n",
    "                new_sent.append((word.lower(), pos))\n",
    "        new_corp.append(new_sent)\n",
    "    return new_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total preprocessing function for a corpus. Input can be one string (corpus),\n",
    "# for which you put multi = False, or a list of several strings (corpora) that \n",
    "# you want to turn into one big corpus, for which you put multi = True.\n",
    "# For PoS tags, put pos = True.\n",
    "def make_file(corp, multi = False, sent = True, pos = False):\n",
    "    if multi:\n",
    "        corpus = ''\n",
    "        for subcorp in corp:\n",
    "            corpus += subcorp\n",
    "    else:\n",
    "        corpus = corp\n",
    "        \n",
    "    if pos:\n",
    "        corpus = part_of_speech(corpus)\n",
    "    \n",
    "    else:\n",
    "        corpus = preprocess(corpus, sent = sent)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns 2 lists of corpora, one from which the ranks will be calculated\n",
    "# and one from which the frequencies will be calculated. Each corpus consists of\n",
    "# a list of tokenized sentences.\n",
    "# Input: corpus that is to be subsampled. Should be a list of tokenized sentences.\n",
    "# k is the amount of tokens that each sampled corpus should contain,\n",
    "# m is the amount of subcorpera you want for both the ranks and frequencies.\n",
    "# Max: I would read Valentin's thesis for an explanation on subsampling\n",
    "def subsampling(corpus, k = 1000000, m = 10, sent = True):\n",
    "    n = len(corpus)\n",
    "    \n",
    "    sen_len = {}\n",
    "\n",
    "    \n",
    "    rank_corpera = []\n",
    "    freq_corpera = []\n",
    "\n",
    "    if sent:\n",
    "        for i in range(m):\n",
    "            used_rank = set()\n",
    "            used_freq = set()\n",
    "            rank_count = 0\n",
    "            freq_count = 0\n",
    "            rank_samples = []\n",
    "            freq_samples = []\n",
    "\n",
    "            while rank_count < k:\n",
    "                index = np.random.randint(n)\n",
    "                if index in used_rank:\n",
    "                    continue\n",
    "\n",
    "                rank_sample = corpus[index]\n",
    "                len_sample = len(rank_sample)\n",
    "\n",
    "                if len_sample == 0:\n",
    "                    continue\n",
    "\n",
    "                rank_samples += rank_sample\n",
    "                rank_count += len_sample\n",
    "\n",
    "                if len_sample not in sen_len and len_sample < 200:\n",
    "                    sen_len[len_sample] = 1\n",
    "                elif len_sample < 200:\n",
    "                    sen_len[len_sample] += 1\n",
    "\n",
    "\n",
    "                used_rank.add(index)\n",
    "\n",
    "            while freq_count < k:\n",
    "                index = np.random.randint(n)\n",
    "                if index in used_freq:\n",
    "                    continue\n",
    "                freq_sample = corpus[index]\n",
    "                len_sample = len(freq_sample)\n",
    "\n",
    "                if len_sample == 0:\n",
    "                    continue\n",
    "\n",
    "                freq_samples += freq_sample\n",
    "                freq_count += len_sample\n",
    "\n",
    "                if len_sample not in sen_len and len_sample < 200:\n",
    "                    sen_len[len_sample] = 1\n",
    "                elif len_sample < 200:\n",
    "                    sen_len[len_sample] += 1\n",
    "\n",
    "                used_freq.add(index)\n",
    "\n",
    "            rank_corpera.append(rank_samples)\n",
    "            freq_corpera.append(freq_samples)\n",
    "#                 rank_corpera.append([item for sublist in rank_samples for item in sublist])\n",
    "#                 freq_corpera.append([item for sublist in freq_samples for item in sublist])\n",
    "\n",
    "\n",
    "    else:\n",
    "        for i in range(m):\n",
    "            rank_samples = random.sample(corpus, k)\n",
    "            freq_samples = random.sample(corpus, k)\n",
    "            rank_corpera.append(rank_samples)\n",
    "            freq_corpera.append(freq_samples)\n",
    "    \n",
    "#     return rank_corpera, freq_corpera, sen_len\n",
    "    return rank_corpera, freq_corpera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe of word frequencies for list of corpora,\n",
    "# with each column corresponding to a different corpus.\n",
    "# Input: list of corpora. Each corpus consists of a list of tokenized sentences.\n",
    "def calculate_freqs(freq_sents):\n",
    "    freq_dict = {}\n",
    "    for i, corpus in enumerate(freq_sents):\n",
    "        freq_dict[i] = collections.Counter(corpus)\n",
    "        \n",
    "    freqs_df = pd.DataFrame(freq_dict)\n",
    "    freqs_df = freqs_df.fillna(0)\n",
    "    \n",
    "    return freqs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe with the mean frequency of each word across different corpora.\n",
    "# Input: frequency dataframe\n",
    "def mean_freqs(freqs_df):\n",
    "    return(freqs_df.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe of word ranks for list of corpora,\n",
    "# with each column corresponding to a different corpus.\n",
    "# Input: list of corpora. Each corpus consists of a list of tokenized sentences.\n",
    "def calculate_ranks(rank_sents):\n",
    "    ranks_dicts = {}\n",
    "    for i, corpus in enumerate(rank_sents):\n",
    "        freqs = collections.Counter(corpus)\n",
    "        ranks_dicts[i] = {w: r for r, (w, c) in enumerate(freqs.most_common(), 1)}\n",
    "        \n",
    "    ranks_df = pd.DataFrame(ranks_dicts)\n",
    "    for column in ranks_df:\n",
    "        min_rank = int(np.ceil(ranks_df[column].max() + 1))\n",
    "        nan_rows = ranks_df[ranks_df[column].isnull()]\n",
    "        num_nans = len(nan_rows)\n",
    "        nan_ranks = list(range(min_rank, min_rank+num_nans))\n",
    "        random.shuffle(nan_ranks)\n",
    "        ranks_df.loc[ranks_df[column].isnull(), column] = nan_ranks\n",
    "\n",
    "    return ranks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe with the mean rank of each word across different corpora.\n",
    "# Input: rank dataframe\n",
    "def mean_ranks(ranks_df):\n",
    "    return ranks_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates combined dataframe of ranks and frequencies\n",
    "# Input: 2 lists (freq_sents and rank_sents) of corpora. Each corpus\n",
    "# consists of a list of tokenized sentences. These lists are to be obtained form\n",
    "# subsampling.\n",
    "def ranks_freqs(freq_sents, rank_sents):\n",
    "    freqs_df = calculate_freqs(freq_sents)\n",
    "    freqs_df['Frequency'] = mean_freqs(freqs_df)\n",
    "    ranks_df = calculate_ranks(rank_sents)\n",
    "    ranks_df['Rank'] = mean_ranks(ranks_df)\n",
    "    \n",
    "    # Put mean ranks and freqs together and remove all words that\n",
    "    # do not have both a rank and frequency (which happens when a word)\n",
    "    # is only present in freq_sents and not in rank_sents or vice versa\n",
    "    ranks_freqs_df = pd.concat([ranks_df, freqs_df], axis = 1)\n",
    "    ranks_freqs_df = ranks_freqs_df.dropna()\n",
    "#     ranks_freqs_df = ranks_freqs_df.loc[ranks_freqs_df['Frequency'] >=1]\n",
    "    return ranks_freqs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_of_freqs_hist_plot(freqs):\n",
    "    freq_of_freqs = collections.Counter(freqs)\n",
    "    plt.bar(list(freq_of_freqs.keys()), freq_of_freqs.values(), color='b')\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_of_freqs_hexbin_plot(freqs):\n",
    "    freq_of_freqs = collections.Counter(freqs)\n",
    "    hexbin_plot(list(freq_of_freqs.keys()), freq_of_freqs.values())\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fano factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fano_factor(ranks_freqs_df, ylim = None):\n",
    "    fano = []\n",
    "\n",
    "    for index, row in ranks_freqs_df.iterrows():\n",
    "        fano.append((np.cov(row[0:10], row[11:21])[0][1])/row['Rank'])\n",
    "    ranks_freqs_df['Fano'] = fano\n",
    "#     fano_plot(ranks_freqs_df, ylim=ylim)\n",
    "    return ranks_freqs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate Zipf's law using $f(w) \\propto \\frac{1}{r(w)^{\\alpha}}$. In the following code block, the $\\alpha$ is found using maximum likelihood estimation, for which the code was taken from https://stats.stackexchange.com/questions/6780/how-to-calculate-zipfs-law-coefficient-from-a-set-of-top-frequencies ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE of Zipf's law parameters (alpha and beta)\n",
    "class Mandelbrot(GenericLikelihoodModel):\n",
    "\n",
    "    def __init__(self, frequencies, ranks, **kwargs):\n",
    "        if not len(frequencies) == len(ranks):\n",
    "            raise ValueError(\"NOT THE SAME NUMBER OF RANKS AND FREQS!\")\n",
    "        \n",
    "        frequencies = np.asarray(frequencies)\n",
    "        ranks = np.asarray(ranks)\n",
    "        \n",
    "        self.n_obs = np.sum(frequencies)\n",
    "        \n",
    "        super().__init__(endog=frequencies, exog=ranks, **kwargs)\n",
    "        self.fit_result = None\n",
    "    \n",
    "\n",
    "    def prob(self, params, ranks=None, log=False):\n",
    "        if ranks is None:\n",
    "            ranks = self.exog\n",
    "        \n",
    "        alpha, beta = params\n",
    "        if log:\n",
    "            return -alpha*lg(beta+ranks) - lg(zeta(alpha, q=beta+1.))\n",
    "        else:\n",
    "            return ((beta + ranks)**(-alpha))/zeta(alpha, q=beta+1.)\n",
    "    \n",
    "    \n",
    "    def loglike(self, params):\n",
    "        rs = self.exog\n",
    "        fs = self.endog\n",
    "        alpha, beta = params\n",
    "        \n",
    "#        if alpha > 10 or beta > 20:\n",
    "#            return -np.inf\n",
    "        \n",
    "#         if alpha < 1.0 or beta < 0.0:\n",
    "#             return -np.inf\n",
    "        \n",
    "        # no need to calculate P(r) when observed f(r) was zero\n",
    "        log_probs = -alpha*lg(beta+rs) - lg(zeta(alpha, q=beta+1.))\n",
    "        log_probs = log_probs.reshape(-1, )\n",
    "        return np.sum(fs * log_probs) - beta**5\n",
    "    \n",
    "    \n",
    "    def register_fit(self, fit_result, overwrite=False):\n",
    "        if not self.fit_result is None and not overwrite:\n",
    "            raise ValueError(\"A fit result is already registered and overwrite=False!\")\n",
    "            \n",
    "        self.fit_result = fit_result\n",
    "        self.optim_params = fit_result.params\n",
    "        self.pseudo_r_squared = self.pseudo_r_squared(self.optim_params)\n",
    "        self.SE, self.SE_relative = fit_result.bse, fit_result.bse/self.optim_params\n",
    "        self.BIC, self.BIC_relative = fit_result.bic,\\\n",
    "                            (-2*self.null_loglike())/fit_result.bic\n",
    "\n",
    "        return self.optim_params\n",
    "    \n",
    "    def print_result(self, string=False):\n",
    "        if self.fit_result is None:\n",
    "            raise ValueError(\"Register a fitting result first!\")\n",
    "\n",
    "        def format_x(x):\n",
    "            return float('{0:.3g}'.format(x))\n",
    "\n",
    "\n",
    "        s = \"=\"*50\n",
    "        s += \"\\n\" + \"MANDELBROT\"\n",
    "        s += \"\\n\" + \"  Optimal Parameters \" + str(tuple(map(format_x, self.optim_params)))\n",
    "        \n",
    "        s += \"\\n\" + \"  Standard Error [relative]: \" + str(tuple(map(format_x, self.SE))) +\\\n",
    "              \", [\" + str(tuple(map(format_x, self.SE_relative))) + \"]\"\n",
    "        \n",
    "        s += \"\\n\" + \"  Pseudo R^2: \" + str(format_x(self.pseudo_r_squared))\n",
    "        \n",
    "        s += \"\\n\" + \"  BIC [relative]: \" + str(format_x(self.BIC)) +\\\n",
    "              \", [\" + str(format_x(self.BIC_relative)) + \"]\"\n",
    "        s += \"\\n\" + \"=\"*50\n",
    "        \n",
    "        if string:\n",
    "            return s\n",
    "        \n",
    "        print(s)\n",
    "    \n",
    "    \n",
    "    def null_loglike(self, epsilon=1e-10):\n",
    "        return self.loglike((1.+epsilon, 0.0))\n",
    "    \n",
    "    def pseudo_r_squared(self, params):\n",
    "        return 1-self.loglike(params)/self.null_loglike()\n",
    "    \n",
    "    \n",
    "    def predict(self, params, ranks=None, freqs=True, n_obs=None, \n",
    "                correct_for_finite_domain=True):\n",
    "        if ranks is None:\n",
    "            ranks = self.exog\n",
    "        ranks = np.asarray(ranks)\n",
    "        \n",
    "        if n_obs is None:\n",
    "            n_obs = self.n_obs\n",
    "            \n",
    "        alpha, beta = params\n",
    "        pred_probs = self.prob(params, ranks=ranks, log=False)\n",
    "        \n",
    "        if correct_for_finite_domain:\n",
    "            if not freqs:\n",
    "                raise NotImplementedError(\"Correction for \"\\\n",
    "                                          \"finite domain not implemented with probabilities!\")\n",
    "            return pred_probs*(n_obs/np.sum(pred_probs))\n",
    "        \n",
    "        if freqs:\n",
    "            return n_obs*pred_probs\n",
    "        \n",
    "        return pred_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe containing the mean frequencies and ranks, as well as \n",
    "# the estimated frequencies from Zipf's law and the error between the (log) mean\n",
    "# frequencies and (log) estimated frequencies.\n",
    "def zipfs_law(df):\n",
    "    mandelbrot = Mandelbrot(df['Frequency'], df['Rank'])\n",
    "    mandelbrot_fit = mandelbrot.fit(start_params=np.asarray([1.0, 1.0]), # [1.0, 1.0]\n",
    "                                method=\"powell\", full_output=True, disp=0)\n",
    "    mandelbrot.register_fit(mandelbrot_fit)\n",
    "    mandelbrot.print_result()\n",
    "    \n",
    "    model_params = mandelbrot.optim_params\n",
    "    alpha, beta =  mandelbrot.optim_params\n",
    "    preds = mandelbrot.predict(model_params, df['Rank'])\n",
    "    df['Estimated frequency'] = preds\n",
    "    df['Rank (log)'] = np.log(df['Rank'])\n",
    "    df['Frequency (log)'] = np.log(df['Frequency'])\n",
    "    df['Estimated frequency (log)'] = np.log(df['Estimated frequency'])\n",
    "    df['Error'] = df['Frequency (log)'] - df['Estimated frequency (log)']\n",
    "    return mandelbrot, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_entropy(alpha, dx=1e-10):\n",
    "    if alpha <= 1.0:\n",
    "        raise ValueError(\"Entropy undefined for the given parameter:\\n\" + \n",
    "                         str(alpha))\n",
    "    return alpha*(-derivative(zeta, alpha, dx=dx))/zeta(alpha) + lg(zeta(alpha))\n",
    "\n",
    "def mandelbrot_entropy(alpha, beta, dx=1e-10):\n",
    "    if alpha <= 1.0 or beta <= 1.0:\n",
    "        raise ValueError(\"Entropy undefined for the given parameters:\\n\" + \n",
    "                         str(alpha) + \" and \" + str(beta))\n",
    "    zeta_b = lambda a: zeta(a, beta+1)\n",
    "    return alpha*(-derivative(zeta_b, alpha, dx=dx))/zeta_b(alpha) + lg(zeta_b(alpha))\n",
    "\n",
    "\n",
    "def neg_log_likelihood(zipf_model, ranks, freqs):\n",
    "    mle_params = zipf_model.optim_params\n",
    "    log_rank_probs = zipf_model.prob(params=mle_params, ranks=ranks, log=True)    \n",
    "    return -freqs*log_rank_probs\n",
    "    \n",
    "    \n",
    "def empirical_entropy(zipf_model, joint_rank_freqs):\n",
    "    rs = list(joint_rank_freqs[\"Rank\"])\n",
    "    fs = list(joint_rank_freqs[\"Frequency\"])\n",
    "    ranks = np.asarray(rs)\n",
    "    freqs = np.asarray(fs)\n",
    "    n = np.sum(freqs)\n",
    "    return (1/n)*np.sum(neg_log_likelihood(zipf_model, ranks, freqs))\n",
    "\n",
    "def typicality(zipf_model, joint_rank_freqs):\n",
    "    mle_params = zipf_model.optim_params\n",
    "    return mandelbrot_entropy(*mle_params) - empirical_entropy(zipf_model, joint_rank_freqs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MANDELBROT\n",
      "  Optimal Parameters (1.14, 3.92)\n",
      "  Standard Error [relative]: (0.000341, 0.0242), [(0.0003, 0.00617)]\n",
      "  Pseudo R^2: 0.653\n",
      "  BIC [relative]: 3680000.0, [2.88]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.166909563683695"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"1000000_26_8.txt\" , encoding=\"utf8\") as handle:\n",
    "    test =  [l.strip() for l in handle.readlines()]\n",
    "\n",
    "\n",
    "sep_corps = [make_file(corpus, multi = True) for corpus in test]\n",
    "\n",
    "sep_corps1 = []\n",
    "for x in sep_corps:\n",
    "    sep_corps1.append(x[0])\n",
    "len(sep_corps1)\n",
    "\n",
    "k = 400000\n",
    "m = 6\n",
    "\n",
    "rank_corpora, freq_corpora = subsampling(sep_corps1, k=k, m=m)\n",
    "ranks_freqs_df = ranks_freqs(rank_corpora, freq_corpora)\n",
    "# Hieruit krijg je dus een dataframe met voor elk woord de rank, frequency,\n",
    "# estimated frequency volgens Zipf's law en de error.\n",
    "mand, df = zipfs_law(ranks_freqs_df)\n",
    "\n",
    "typicality(mand, ranks_freqs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MANDELBROT\n",
      "  Optimal Parameters (1.19, 3.06)\n",
      "  Standard Error [relative]: (0.000503, 0.0265), [(0.000423, 0.00866)]\n",
      "  Pseudo R^2: 0.708\n",
      "  BIC [relative]: 2900000.0, [3.43]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.824719672035206"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"1000000_2_0.txt\" , encoding=\"utf8\") as handle:\n",
    "    test =  [l.strip() for l in handle.readlines()]\n",
    "\n",
    "\n",
    "sep_corps = [make_file(corpus, multi = True) for corpus in test]\n",
    "\n",
    "sep_corps1 = []\n",
    "for x in sep_corps:\n",
    "    sep_corps1.append(x[0])\n",
    "len(sep_corps1)\n",
    "\n",
    "k = 400000\n",
    "m = 6\n",
    "\n",
    "rank_corpora, freq_corpora = subsampling(sep_corps1, k=k, m=m)\n",
    "ranks_freqs_df = ranks_freqs(rank_corpora, freq_corpora)\n",
    "# Hieruit krijg je dus een dataframe met voor elk woord de rank, frequency,\n",
    "# estimated frequency volgens Zipf's law en de error.\n",
    "mand, df = zipfs_law(ranks_freqs_df)\n",
    "\n",
    "typicality(mand, ranks_freqs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MANDELBROT\n",
      "  Optimal Parameters (1.18, 3.17)\n",
      "  Standard Error [relative]: (0.000463, 0.0267), [(0.000394, 0.00841)]\n",
      "  Pseudo R^2: 0.696\n",
      "  BIC [relative]: 3060000.0, [3.29]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.055302335405379"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"1000000_10_8.txt\" , encoding=\"utf8\") as handle:\n",
    "    test =  [l.strip() for l in handle.readlines()]\n",
    "\n",
    "\n",
    "sep_corps = [make_file(corpus, multi = True) for corpus in test]\n",
    "\n",
    "sep_corps1 = []\n",
    "for x in sep_corps:\n",
    "    sep_corps1.append(x[0])\n",
    "len(sep_corps1)\n",
    "\n",
    "k = 400000\n",
    "m = 6\n",
    "\n",
    "rank_corpora, freq_corpora = subsampling(sep_corps1, k=k, m=m)\n",
    "ranks_freqs_df = ranks_freqs(rank_corpora, freq_corpora)\n",
    "# Hieruit krijg je dus een dataframe met voor elk woord de rank, frequency,\n",
    "# estimated frequency volgens Zipf's law en de error.\n",
    "mand, df = zipfs_law(ranks_freqs_df)\n",
    "\n",
    "typicality(mand, ranks_freqs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "MANDELBROT\n",
      "  Optimal Parameters (1.15, 3.58)\n",
      "  Standard Error [relative]: (0.00038, 0.0259), [(0.000331, 0.00723)]\n",
      "  Pseudo R^2: 0.667\n",
      "  BIC [relative]: 3460000.0, [3.01]\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.708401525222168"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"1000000_22_3.txt\" , encoding=\"utf8\") as handle:\n",
    "    test =  [l.strip() for l in handle.readlines()]\n",
    "\n",
    "\n",
    "sep_corps = [make_file(corpus, multi = True) for corpus in test]\n",
    "\n",
    "sep_corps1 = []\n",
    "for x in sep_corps:\n",
    "    sep_corps1.append(x[0])\n",
    "len(sep_corps1)\n",
    "\n",
    "k = 400000\n",
    "m = 6\n",
    "\n",
    "rank_corpora, freq_corpora = subsampling(sep_corps1, k=k, m=m)\n",
    "ranks_freqs_df = ranks_freqs(rank_corpora, freq_corpora)\n",
    "# Hieruit krijg je dus een dataframe met voor elk woord de rank, frequency,\n",
    "# estimated frequency volgens Zipf's law en de error.\n",
    "mand, df = zipfs_law(ranks_freqs_df)\n",
    "\n",
    "typicality(mand, ranks_freqs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
